{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import feedparser\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting papers...\n",
      "http://export.arxiv.org/api/query?search_query=all=the&start=0&max_results=1000\n",
      "Total number retrieved: 1000\n",
      "http://export.arxiv.org/api/query?search_query=all=the&start=1000&max_results=1000\n",
      "Total number retrieved: 2000\n",
      "http://export.arxiv.org/api/query?search_query=all=the&start=2000&max_results=1000\n",
      "Total number retrieved: 3000\n",
      "http://export.arxiv.org/api/query?search_query=all=the&start=3000&max_results=1000\n",
      "Total number retrieved: 4000\n",
      "http://export.arxiv.org/api/query?search_query=all=the&start=4000&max_results=1000\n",
      "Total number retrieved: 5000\n",
      "Papers retrieved.\n"
     ]
    }
   ],
   "source": [
    "# Let's get abstracts from 1000 recent papers for each of the following 6 arXiv categories:\n",
    "# --> Physics, Mathematics, Computer Science, Quantitative Biology, Statistics, Economics\n",
    "\n",
    "#### ---- https://arxiv.org/help/api/user-manual---- ####\n",
    "# In cases where the API needs to be called multiple times in a row, we encourage you to \n",
    "# play nice and incorporate a 3 second delay in your code. The detailed examples below \n",
    "# illustrate how to do this in a variety of languages.\n",
    "# Note\tBecause of speed limitations in our implementation of the API, the maximum number \n",
    "# of results returned from a single call (max_results) is limited to 30000 in slices of at \n",
    "# most 2000 at a time, using the max_results and start query parameters.\n",
    "\n",
    "def get_arXiv_papers(search_term, num_calls=5, max_results=1000):\n",
    "    '''\n",
    "    USAGE:\n",
    "    Get info from the arXiv for a number (default 5000) of papers matching\n",
    "    a specified search term.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    search_term - term to search in the arXiv\n",
    "    num_calls - number of calls to make to arXiv API\n",
    "    max_results - maximum number of results per call\n",
    "    \n",
    "    RETURNS:\n",
    "    entries - list of results from the request\n",
    "    '''\n",
    "\n",
    "    print('Getting papers...')\n",
    "    url_base = 'http://export.arxiv.org/api/query?search_query='\n",
    "    entries = []\n",
    "\n",
    "    i = 0\n",
    "    while i < num_calls:\n",
    "        start = i * max_results\n",
    "        assert start + max_results <= 30000\n",
    "        url_query = 'all=%s&start=%d&max_results=%d'% (search_term, start, max_results)\n",
    "        url = url_base + url_query\n",
    "        print(url)\n",
    "\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Sometimes fails, but returns status_code 200 anyway. Not sure why.\n",
    "        # Make sure that new data is really retrieved... \n",
    "        \n",
    "        before = len(entries)\n",
    "        entries += feedparser.parse(response.content).entries\n",
    "        after = len(entries)\n",
    "\n",
    "        # If we successfully get new papers...\n",
    "        if after > before:\n",
    "            i += 1\n",
    "\n",
    "        print('Total number retrieved: %d' % len(entries))\n",
    "\n",
    "        time.sleep(3) # Wait 3 seconds between requests\n",
    "\n",
    "    print('Papers retrieved.')\n",
    "    return entries\n",
    "    \n",
    "entries = get_arXiv_papers('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file...\n",
      "Data written to \"papers_from_arXiv\\arXiv_papers.json\".\n"
     ]
    }
   ],
   "source": [
    "folder = 'papers_from_arXiv'\n",
    "file_name = 'arXiv_papers.json'\n",
    "file_path = os.path.join(folder, file_name)\n",
    "\n",
    "def write_to_JSON(data, file_path):\n",
    "\n",
    "    print('Writing to file...')\n",
    "    with open(file_path, 'w') as fp:\n",
    "        json.dump(data, fp)\n",
    "    print('Data written to \"%s\".' % file_path)\n",
    "\n",
    "write_to_JSON(entries, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file...\n",
      "DataFrame generated from \"papers_from_arXiv\\arXiv_papers.json\".\n"
     ]
    }
   ],
   "source": [
    "def load_papers(file_path):\n",
    "    '''\n",
    "    USAGE: \n",
    "    Load paper info from file and generate a DataFrame and list of categories.\n",
    "    \n",
    "    ARGUMENT: \n",
    "    file_path - location of file\n",
    "    \n",
    "    RETURNS: \n",
    "    terms - list of paper categories\n",
    "    df - Pandas DataFrame with columns ['term', 'title', 'summary', 'id']\n",
    "    '''\n",
    "    \n",
    "    print('Loading data from file...')\n",
    "    with open(file_path, 'r') as fp:\n",
    "        entries = json.load(fp)\n",
    "\n",
    "    df = pd.DataFrame(entries)\n",
    "    terms = []\n",
    "    for k in range(len(df.arxiv_primary_category)):\n",
    "        term = df.iloc[k].arxiv_primary_category['term']\n",
    "        terms.append(term.split('.')[0])\n",
    "        \n",
    "    df['term'] = terms\n",
    "    df = df[['term', 'title', 'summary', 'id']]\n",
    "    \n",
    "    print('DataFrame generated from \"%s\".' % file_path)\n",
    "    return terms, df\n",
    "\n",
    "terms, df = load_papers(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_top_terms(terms, num=10):\n",
    "    '''\n",
    "    USAGE: Make a list of top paper categories.\n",
    "    \n",
    "    ARGUMENTS: \n",
    "    terms - list of all paper categories (\"terms\")\n",
    "    num - number of categories to keep\n",
    "    \n",
    "    RETURNS:\n",
    "    top_terms - list of names of most frequent categories\n",
    "    '''\n",
    "\n",
    "    term_dict = {}\n",
    "    for term in terms:\n",
    "        if term in term_dict:\n",
    "            term_dict[term] += 1\n",
    "        else:\n",
    "            term_dict[term] = 1\n",
    "\n",
    "    term_dict_rev = dict(zip(list(term_dict.values()), list(term_dict.keys())))\n",
    "    counts_thresh = sorted(list(term_dict_rev.keys()))[-num]\n",
    "    top_terms = [term_dict_rev[x] for x in term_dict_rev if x >= counts_thresh]\n",
    "    return top_terms\n",
    "\n",
    "top_terms = get_top_terms(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prune_df(df, top_terms):\n",
    "    '''\n",
    "    USAGE: Prune DataFrame to include papers only from most common categories\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    df - DataFrame of paper info\n",
    "    top_terms - list of most frequent categories\n",
    "    \n",
    "    RETURNS:\n",
    "    pruned_df - pruned paper DataFrame\n",
    "    '''\n",
    "    pruned_df = df[df['term'].isin(top_terms)]\n",
    "    return pruned_df\n",
    "\n",
    "df = prune_df(df, top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting word stems in 4583 abstracts...\n",
      "Stems counted.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def abstract2df(df_small, idx):\n",
    "    '''\n",
    "    USAGE: Use Porter Stemming to process a paper abstract.\n",
    "    \n",
    "    ARGUMENT: \n",
    "    df - DataFrame with keys [\"term\", \"title\", \"summary\", \"id\"]\n",
    "    idx - index locating paper within DataFrame\n",
    "    \n",
    "    RETURNS: stem_df - DataFrame with keys [\"stems\", \"counts\", \"docs\"]\n",
    "    '''\n",
    "    abstract = df.iloc[idx].summary\n",
    "    words = word_tokenize(abstract)\n",
    "    stems = [ps.stem(w).lower() for w in words]\n",
    "    counts = Counter(stems)\n",
    "    docs = list(np.ones(len(counts)).astype(int))\n",
    "    stem_df = pd.DataFrame({'stems': list(counts.keys()), 'counts': list(counts.values()), 'docs': docs})\n",
    "    stem_df = stem_df[['stems', 'counts', 'docs']]\n",
    "    return stem_df\n",
    "\n",
    "def count_stems(df):\n",
    "    '''\n",
    "    USAGE:\n",
    "    Count up total number of occurrences of each word stem that appears in the df \n",
    "    DataFrame, and count up the numer of documents each of those word stems appears in.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    df - DataFrame with keys [\"term\", \"title\", \"summary\", \"id\"]\n",
    "    \n",
    "    RETURNS:\n",
    "    full_stem_df - DataFrame with keys [\"stems\", \"counts\", \"docs\"] sorted by \"counts\"\n",
    "    '''\n",
    "    \n",
    "    print('Counting word stems in %d abstracts...' % len(df))\n",
    "    full_stem_df = pd.DataFrame()\n",
    "    for idx in range(len(df)):\n",
    "        stem_df = abstract2df(df, idx)\n",
    "        full_stem_df = pd.concat([full_stem_df, stem_df])\n",
    "        if idx and idx % 10 == 0 or idx == len(df) - 1:\n",
    "            full_stem_df = full_stem_df.groupby('stems').sum()\n",
    "            full_stem_df['stems'] = full_stem_df.index\n",
    "            full_stem_df = full_stem_df[['stems', 'counts', 'docs']]\n",
    "    \n",
    "    full_stem_df.sort_values('counts', ascending=False, inplace=True)\n",
    "    print('Stems counted.')\n",
    "    return full_stem_df\n",
    "\n",
    "all_stems = count_stems(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def docs2idf(df, num):\n",
    "    '''\n",
    "    USAGE: \n",
    "    Get an inverse document frequency (idf) measure for each stem, \n",
    "    and add that measure to the word stem DataFrame.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    df - word stem DataFrame\n",
    "    num - total number of papers in document pool\n",
    "    '''\n",
    "    df['idf'] = np.log(num/df['docs'])\n",
    "    return df\n",
    "\n",
    "all_stems = docs2idf(all_stems, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding paper abstracts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 4583/4583 [02:20<00:00, 32.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper abstracts converted to vectors.\n"
     ]
    }
   ],
   "source": [
    "def papers2matrix(stems, samples):\n",
    "    '''\n",
    "    USAGE:\n",
    "    Embed a selection of paper abstracts in a high-dimensional space where each \n",
    "    dimension represents the \"term frequency-inverse document frequency\" (tf-idf) of a \n",
    "    given word stem. The tf-idf is a product of the {relative frequency with which a \n",
    "    term appears in a single document} and the log of the {total number of papers in\n",
    "    the document pool divided by the number of documents in which a term appears}.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    stems - DataFrame of word stems in our dataset \n",
    "    samples - number of abstracts to embed\n",
    "    \n",
    "    RETURNS:\n",
    "    matrix - 2D array of embedded paper-abstract row vectors\n",
    "    info - list of ['id', 'term', 'title'] lists corresponding to each paper embedded\n",
    "    '''\n",
    "    \n",
    "    assert samples <= len(df)\n",
    "    \n",
    "    idf = np.array(list(stems['idf']))\n",
    "    stems = stems[['stems', 'idf']]\n",
    "    matrix = np.zeros((samples, len(stems)))\n",
    "    info = []\n",
    "    \n",
    "    print('Embedding paper abstracts...')\n",
    "    \n",
    "    i = 0\n",
    "    for idx in tqdm(range(samples)):\n",
    "    \n",
    "        information = list(df.iloc[idx][['id', 'term', 'title']])\n",
    "        info.append(information)\n",
    "                \n",
    "        new_df = pd.merge(stems, abstract2df(df, idx), on='stems', how='left').fillna(0)\n",
    "        \n",
    "        vec = np.array(list(new_df['counts']))\n",
    "        vec /= np.sum(vec) # components sum to 1\n",
    "        vec *= idf         # apply inverse document frequency\n",
    "        matrix[i, :] = vec\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    print('Paper abstracts converted to vectors.')\n",
    "    \n",
    "    return matrix, info\n",
    "\n",
    "matrix, info = papers2matrix(all_stems, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Truncated SVD...\n"
     ]
    }
   ],
   "source": [
    "## Note: dimensionality reduction --> PCA for dense data, TruncatedSVD for sparse data\n",
    "## (http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from time import time\n",
    "\n",
    "def reduce_dim(matrix, dims):\n",
    "    '''\n",
    "    USAGE: Perform Truncated SVD to reduce dimensionality of embedding space.\n",
    "    \n",
    "    ARGUMENTS: \n",
    "    matrix - training data (sparse matrix) with shape (n_features, n_samples)\n",
    "    dims - number of dimensions to project data into\n",
    "    \n",
    "    RETURNS:\n",
    "    X_new - array of reduced dimensionality\n",
    "    '''\n",
    "    \n",
    "    tic =time()\n",
    "    print('Performing Truncated SVD...')\n",
    "    svd = TruncatedSVD(n_components = dims, random_state = 1)\n",
    "    X = svd.fit_transform(matrix) # matrix.shape = (n_samples, n_features)\n",
    "    toc = time()\n",
    "    print('Embeddings reduced from %d to %d dimensions through TruncatedSVD. (Time elapsed: %.2f s)' \n",
    "          % (matrix.shape[1], dims, (toc-tic)))\n",
    "    return X\n",
    "\n",
    "# with X_embedded.shape = (5000, 128), perplexity = 30, elapsed TSNE time ~ 5 minutes.\n",
    "def TSNE2D(X):\n",
    "    '''\n",
    "    USAGE: Perform TSNE to reduce embedding space to 2D\n",
    "    ARGUMENT: X - high-dimensional training array (n_samples, n_features ~ 100)\n",
    "    RETURNS: X_embedded - 2D matrix (n_samples, 2)\n",
    "    '''\n",
    "    \n",
    "    tic = time()\n",
    "    print('Performing TSNE...')\n",
    "    X_embedded = TSNE(n_components=2, perplexity=30, random_state = 1).fit_transform(X)\n",
    "    toc = time()\n",
    "    print('Embeddings reduced to 2 dimensions through TSNE. (Time elapsed: %.2f s)' % (toc-tic))\n",
    "    return X_embedded\n",
    "\n",
    "X = reduce_dim(matrix, 128)\n",
    "X_embedded = TSNE2D(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_embedded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-eeaefadbad64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mplot_2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_embedded' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_2D(X, info):\n",
    "    '''\n",
    "    USAGE: \n",
    "    Create scatter plot of points in array with shape (n_samples, 2)\n",
    "    \n",
    "    ARGUMENTS: \n",
    "    X - 2D array of shape (n_samples, 2)\n",
    "    info - list of ['id', 'term', 'title'] lists corresponding to each paper embedded \n",
    "    '''\n",
    "    x = X[:, 0]\n",
    "    y = X[:, 1]\n",
    "    \n",
    "    term_list = list(np.array(info).T[1])\n",
    "    term_set = list(set(term_list))\n",
    "    term_list = [term_set.index(term) for term in term_list]\n",
    "    \n",
    "    color_list = plt.cm.tab10(term_list)\n",
    "    \n",
    "    plt.figure(figsize = (15,15))\n",
    "    plt.scatter(x,y, s = 10, c = color_list)\n",
    "    plt.show()\n",
    "    \n",
    "plot_2D(X_embedded, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data exported to \"papers_from_arXiv\\processed\\embedding_0.json\".\n"
     ]
    }
   ],
   "source": [
    "name = 'embedding'\n",
    "destination = os.path.join('papers_from_arXiv', 'processed', name + '_0.json')\n",
    "\n",
    "def get_filepath(filepath):\n",
    "    '''\n",
    "    USAGE: Iterate filepath suffix index up by 1 if filename is already taken.\n",
    "    ARGUMENT: filepath - string with format (root + num + .extension)\n",
    "    RETURNS: returns filepath if it doesn't exist, otherwise, executes function again.\n",
    "    '''\n",
    "    if not os.path.exists(filepath):\n",
    "        return filepath \n",
    "    else:\n",
    "        root = \"_\".join(filepath.split('_')[:-1]) + '_'\n",
    "        num = int(filepath.split('_')[-1].split('.')[0])\n",
    "        ext = '.' + filepath.split('.')[1]\n",
    "        new_path = '%s%d%s' % (root, num + 1, ext)\n",
    "        return get_filepath(new_path)\n",
    "\n",
    "def export_data(X, info, destination):\n",
    "    '''\n",
    "    USAGE: \n",
    "    Export embeddings and their info to JSON file.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "    X - list of [x, y] lists of coordinates\n",
    "    info - list of ['id', 'term', 'title'] lists corresponding to each paper embedded\n",
    "    '''\n",
    "    destination = get_filepath(destination)\n",
    "    categories = dict(zip(top_terms, [x for x in range(len(top_terms))]))\n",
    "    data = {'X': X.tolist(), 'info': info, 'categories': categories}\n",
    "    with open(destination, 'w') as fp:\n",
    "        json.dump(data, fp)\n",
    "    print('Processed data exported to \"%s\".' % destination)\n",
    "    \n",
    "export_data(X_embedded, info, destination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
